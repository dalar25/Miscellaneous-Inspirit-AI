{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "578cd7448e47453dbc66842cb2c5a4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90db5f9db75c4f68b0f82054684716c2",
              "IPY_MODEL_8da18464d9144b4c93100b4079ab6892"
            ],
            "layout": "IPY_MODEL_97181fe8172e49f189109a0e7f11c4fb"
          }
        },
        "90db5f9db75c4f68b0f82054684716c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "text",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_11a8d2cf0b564204894baf30769ca434",
            "placeholder": "​",
            "style": "IPY_MODEL_03d93c15f11747c591e6e2c01516e8f5",
            "value": "abc"
          }
        },
        "8da18464d9144b4c93100b4079ab6892": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6e74d26c9fd84573abb5da72b85bf8d3",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0]])"
                },
                "metadata": {}
              }
            ]
          }
        },
        "97181fe8172e49f189109a0e7f11c4fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a8d2cf0b564204894baf30769ca434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03d93c15f11747c591e6e2c01516e8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e74d26c9fd84573abb5da72b85bf8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "780d1c9b89c54abcbe8a403f7ac30fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4dc0594b7f7d47e69c410684b00de821",
              "IPY_MODEL_1a791db9885742a9899dd178b04d6b96"
            ],
            "layout": "IPY_MODEL_34c10b61a4784aa585ce401ce2fee884"
          }
        },
        "4dc0594b7f7d47e69c410684b00de821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "sequence",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4d02c4b047154e3ea7efc4375c2fa9d7",
            "placeholder": "​",
            "style": "IPY_MODEL_7330b61a5f5f4a0299315bc923c1bedb",
            "value": ""
          }
        },
        "1a791db9885742a9899dd178b04d6b96": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b726418efff14c72ba93653bdaaffdf0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "waiting...\n"
                ]
              }
            ]
          }
        },
        "34c10b61a4784aa585ce401ce2fee884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d02c4b047154e3ea7efc4375c2fa9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7330b61a5f5f4a0299315bc923c1bedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b726418efff14c72ba93653bdaaffdf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5427ee4800fb4503bdc5c1a3d1adca96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9f8de7f25d1462bbd9a4e04c1a9f14f",
              "IPY_MODEL_6e9504536f8740b497e00216a26c8ccb"
            ],
            "layout": "IPY_MODEL_ae54204c19c54567af6a501ede66f0ca"
          }
        },
        "c9f8de7f25d1462bbd9a4e04c1a9f14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "sequence",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7a99b56f5f2b4f8f86181690e4fc0cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_76c659a2219d49b59b563cb584ee2868",
            "value": "th"
          }
        },
        "6e9504536f8740b497e00216a26c8ccb": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_72755d2df1574b49aca2725d82b11a39",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<Figure size 432x288 with 1 Axes>",
                  "image/png": "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\n"
                },
                "metadata": {
                  "needs_background": "light"
                }
              }
            ]
          }
        },
        "ae54204c19c54567af6a501ede66f0ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a99b56f5f2b4f8f86181690e4fc0cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76c659a2219d49b59b563cb584ee2868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72755d2df1574b49aca2725d82b11a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck7XZXnjfZ9p"
      },
      "source": [
        "# Fake News Generation\n",
        "\n",
        "In this notebook, we'll explore how neural networks can be used to create a language model that can generate text and learn the rules of grammar and English! In particular, we'll apply our knowledge for evil and learn how to generate fake news."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx9IoN6bibZj"
      },
      "source": [
        "**Before starting, set your runtype type to GPU!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfUgfksN_iZR"
      },
      "source": [
        "##Outline\n",
        "\n",
        "We'll build RNNs to predict language character-by-character to generate fake news! We'll:\n",
        "\n",
        "\n",
        "* Encode our text data for the language model\n",
        "* Build, train, and explore RNN and LSTM models\n",
        "* Advanced: Create visualizations of our model's confidence\n",
        "* Optional: compare our results to a state of the art word-wise language model, GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-THemqM_Uy_C",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092bac1b-a531-4219-d501-74c5ef7c22be"
      },
      "source": [
        "#@title Run this cell to import libraries and download the data! If there is a prompt, just enter \"A\"\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "from collections import Counter\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!wget \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%201%20-%2010%20(Main%20Curriculum)/Session%203_%20NLP%20and%20Sequences_%20RNNs%2C%20LSTMs/fake.txt\"\n",
        "!wget \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%201%20-%2010%20(Main%20Curriculum)/Session%203_%20NLP%20and%20Sequences_%20RNNs%2C%20LSTMs/pre_train.zip\"\n",
        "\n",
        "! unzip -oq pre_train.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-23 03:34:36--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%201%20-%2010%20(Main%20Curriculum)/Session%203_%20NLP%20and%20Sequences_%20RNNs%2C%20LSTMs/fake.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.188.48, 172.217.164.176, 142.251.45.16, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.188.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 300000 (293K) [text/plain]\n",
            "Saving to: ‘fake.txt’\n",
            "\n",
            "\rfake.txt              0%[                    ]       0  --.-KB/s               \rfake.txt            100%[===================>] 292.97K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-23 03:34:36 (16.9 MB/s) - ‘fake.txt’ saved [300000/300000]\n",
            "\n",
            "--2022-04-23 03:34:36--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/Deep%20Dives/Advanced%20Topics%20in%20AI/Sessions%201%20-%2010%20(Main%20Curriculum)/Session%203_%20NLP%20and%20Sequences_%20RNNs%2C%20LSTMs/pre_train.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.16.128, 142.251.45.112, 142.250.188.48, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.16.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 638112 (623K) [application/zip]\n",
            "Saving to: ‘pre_train.zip’\n",
            "\n",
            "pre_train.zip       100%[===================>] 623.16K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-23 03:34:36 (27.7 MB/s) - ‘pre_train.zip’ saved [638112/638112]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGFprDdkVJFd",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to load some helper functions\n",
        "def load_data():\n",
        "    with open(\"fake.txt\", \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def simplify_text(text, vocab):\n",
        "    new_text = \"\"\n",
        "    for ch in text:\n",
        "        if ch in vocab:\n",
        "            new_text += ch\n",
        "    return new_text\n",
        "\n",
        "def sample_from_model(\n",
        "    model,\n",
        "    text,\n",
        "    char_indices,\n",
        "    chunk_length,\n",
        "    number_of_characters,\n",
        "    seed=\"\",\n",
        "    generation_length=400,\n",
        "):\n",
        "    indices_char = {v: k for k, v in char_indices.items()}\n",
        "    for diversity in [0.2, 0.5, 0.7]:\n",
        "        print(\"----- diversity:\", diversity)\n",
        "        generated = \"\"\n",
        "        if not seed:\n",
        "            text = text.lower()\n",
        "            start_index = random.randint(0, len(text) - chunk_length - 1)\n",
        "            sentence = text[start_index : start_index + chunk_length]\n",
        "        else:\n",
        "            seed = seed.lower()\n",
        "            sentence = seed[:chunk_length]\n",
        "            sentence = \" \" * (chunk_length - len(sentence)) + sentence\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for _ in range(generation_length):\n",
        "            x_pred = np.zeros((1, chunk_length, number_of_characters))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\") + 1e-8\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "class SampleAtEpoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, data, char_indices, chunk_length, number_of_characters):\n",
        "        self.data = data\n",
        "        self.char_indices = char_indices\n",
        "        self.chunk_length = chunk_length\n",
        "        self.number_of_characters = number_of_characters\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        sample_from_model(\n",
        "            self.model,\n",
        "            self.data,\n",
        "            self.char_indices,\n",
        "            self.chunk_length,\n",
        "            self.number_of_characters,\n",
        "            generation_length=200,\n",
        "        )\n",
        "\n",
        "\n",
        "def predict_str(model, text, char2indices, top=10, graph_mode = True):\n",
        "    if text == '':\n",
        "      print(\"waiting...\")\n",
        "      return\n",
        "    text = text.lower()\n",
        "    assert len(text) <= CHUNK_LENGTH\n",
        "    oh = np.array([one_hot_sentence(text, char2indices)])\n",
        "    with warnings.catch_warnings():\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      pred = model.predict(oh).flatten()\n",
        "    sort_indices = np.argsort(pred)[::-1][:top]\n",
        "    if graph_mode:\n",
        "      plt.bar(range(top), pred[sort_indices], tick_label=np.array(list(VOCAB))[sort_indices])\n",
        "      plt.title(f\"Predicted probabilities of the character following '{text}'\")\n",
        "      plt.show()\n",
        "    else:\n",
        "      return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvr9mfPLgHZf"
      },
      "source": [
        "## Language models\n",
        "\n",
        "A language model tries to learn how language works. Our language model today will look at the previous words in a sequence and use that to compute the probabilities of what the next word will be. Actually, our model will do something even more fundamental: it'll try to predict what the next character in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6RTa9-2U-sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e624bbc-9e6a-46c4-8cef-d075105b1a08"
      },
      "source": [
        "#@title Run to load the vocabulary\n",
        "\n",
        "# VOCABULARY defines the set of acceptable characters that the model can handle\n",
        "# CORPUS_LENGTH is how long our training dataset is\n",
        "# CHUNK_LENGTH is how many characters previously our model can remember\n",
        "# CHAR2INDICES is a mapping from characters to their indices in the one hot encoding\n",
        "\n",
        "STEP = 3\n",
        "LEARNING_RATE = 0.0005\n",
        "CORPUS_LENGTH = 200000\n",
        "CHUNK_LENGTH = 40\n",
        "VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "CHAR2INDICES = dict(zip(VOCAB, range(len(VOCAB))))\n",
        "print(VOCAB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5N4kVBHivkR"
      },
      "source": [
        "Let's start by loading in the data and simplifying the text a bit by removing all the characters that are not in our vocabulary. Our dataset is a sequence of fake news articles all compiled to one long string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xNZ-FRjVJDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175c0c5e-5348-463b-e3c1-9d00499957ec"
      },
      "source": [
        "data = load_data()\n",
        "data = data[:CORPUS_LENGTH]\n",
        "data = simplify_text(data, CHAR2INDICES)\n",
        "print(f\"Type of the data is: {type(data)}\\n\")\n",
        "print(f\"Length of the data is: {len(data)}\\n\")\n",
        "print(f\"The first couple of sentences of the data are:\\n\")\n",
        "print(data[0:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the data is: <class 'str'>\n",
            "\n",
            "Length of the data is: 200000\n",
            "\n",
            "The first couple of sentences of the data are:\n",
            "\n",
            "print they should pay all the back all the money plus interest. the entire family and everyone who came in with them need to be deported asap. why did it take two years to bust them? \n",
            "here we go again another group stealing from the government and taxpayers! a group of somalis stole over four million in government benefits over just 10 months! \n",
            "weve reported on numerous cases like this one where the muslim refugees/immigrants commit fraud by scamming our systemits way out of control! more relate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8xSZrQZ_ugW"
      },
      "source": [
        "## Discussion 1\n",
        "\n",
        "What does `len(data)` tell us?\n",
        "\n",
        "a. number of sentences in our data\n",
        "\n",
        "b. number of words in our data\n",
        "\n",
        "**c. number of characters in our data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5u33MrnjcXj"
      },
      "source": [
        "## Encoding words\n",
        "\n",
        "Before we can do any machine learning, we'll have to encode our data in numbers. Just like in the Yelp review notebook, we'll be using one hot encodings - but with two differences:\n",
        "\n",
        "1. This time, the vocabulary is the set of characters instead of words. \n",
        "\n",
        "2. In text generation, we care a lot about context/order - so we won't use the Bag of Words model, where we just add up the one hot vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezO-hpg8rb4K"
      },
      "source": [
        "### Exercise 1\n",
        "We want to make a one-hot vector for a given character.  For example, the one-hot encoding for 'b' is:\n",
        "\n",
        "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJuumbw4S7wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4baeccd-693e-4b67-f114-04f6af71e748"
      },
      "source": [
        "print(CHAR2INDICES)\n",
        "#How does this help us?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '!': 26, '\"': 27, '#': 28, '$': 29, '%': 30, '&': 31, \"'\": 32, '(': 33, ')': 34, '*': 35, '+': 36, ',': 37, '-': 38, '.': 39, '/': 40, ':': 41, ';': 42, '<': 43, '=': 44, '>': 45, '?': 46, '@': 47, '[': 48, '\\\\': 49, ']': 50, '^': 51, '_': 52, '`': 53, '{': 54, '|': 55, '}': 56, '~': 57, '0': 58, '1': 59, '2': 60, '3': 61, '4': 62, '5': 63, '6': 64, '7': 65, '8': 66, '9': 67, ' ': 68, '\\n': 69}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwN7fo1IWlgL"
      },
      "source": [
        "def one_hot(char, char_indices): #char_indices arg will be fill by CHAR2INDICES, shown above\n",
        "    num_chars = len(char_indices)\n",
        "    vec = [0] * num_chars # Start off with a vector of all 0s\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    # Your task: where in vec does the 1 go?\n",
        "    vec[char_indices[char]] = 1\n",
        "    ### END YOUR CODE ###\n",
        "    return vec\n",
        "\n",
        "\n",
        "def one_hot_sentence(sentence, char_indices):\n",
        "    return [one_hot(c, char_indices) for c in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjNBrFRklFuA"
      },
      "source": [
        "When you've got it, test it below, try typing 'abc', and see if you get what you would expect!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BouniNa5lE44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "578cd7448e47453dbc66842cb2c5a4d6",
            "90db5f9db75c4f68b0f82054684716c2",
            "8da18464d9144b4c93100b4079ab6892",
            "97181fe8172e49f189109a0e7f11c4fb",
            "11a8d2cf0b564204894baf30769ca434",
            "03d93c15f11747c591e6e2c01516e8f5",
            "6e74d26c9fd84573abb5da72b85bf8d3"
          ]
        },
        "outputId": "aa64e0af-8a4c-4234-acb0-3368e1751e77"
      },
      "source": [
        "interact(lambda text: np.array(one_hot_sentence(text, CHAR2INDICES)), text=\"abc\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(Text(value='abc', description='text'), Output()), _dom_classes=('widget-interact',))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "578cd7448e47453dbc66842cb2c5a4d6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngmxrKtC9I03"
      },
      "source": [
        "### Exercise 2\n",
        "Let's make sure we understand what one_hot_sentence is doing by printing its shape and figuring out what the dimensions mean - a common practice in coding and debugging!\n",
        "\n",
        "Print the dimensions of abc_encoded.  What do they mean?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EmK7jbx9brZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce364af6-8b13-4300-d889-c4f5ee043f10"
      },
      "source": [
        "abc_encoded = np.array(one_hot_sentence('abc', CHAR2INDICES))\n",
        "print(abc_encoded)\n",
        "abc_encoded.shape\n",
        "### your code here ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 70)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEwPDLvsmdEk"
      },
      "source": [
        "## Building the Language Model\n",
        "\n",
        "We'll use a LSTM for our language model, which is a neural network that specializes in sequences. [Check this link out for an explanation of LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seiOTRwNWrPZ"
      },
      "source": [
        "#@title Run to extract x and y, the input and output to the model, from the raw text.\n",
        "def get_x_y(text, char_indices):\n",
        "    \"\"\"\n",
        "    Extracts x and y from the raw text.\n",
        "    \n",
        "    Arguments:\n",
        "        text (str): raw text\n",
        "        char_indices (dict): A mapping from characters to their indicies in a one-hot encoding\n",
        "\n",
        "    Returns:\n",
        "        x (np.array) with shape (num_sentences, max_len, size_of_vocab)\n",
        "    \n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(text) - CHUNK_LENGTH, STEP):\n",
        "        sentences.append(text[i : i + CHUNK_LENGTH])\n",
        "        next_chars.append(text[i + CHUNK_LENGTH])\n",
        "\n",
        "    print(\"Chunk length:\", CHUNK_LENGTH)\n",
        "    print (\"Step size:\", STEP)\n",
        "    print(\"Number of chunks:\", len(sentences))\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        x.append(one_hot_sentence(sentence, char_indices))\n",
        "        y.append(one_hot(next_chars[i], char_indices))\n",
        "\n",
        "    return np.array(x, dtype=bool), np.array(y, dtype=bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmonfcQnlW0U"
      },
      "source": [
        "Let's check out `x` and `y`! Remember that we're trying to predict the next character given the previous CHUNK_LENGTH characters, and that each character is represented by a vector of length VOCAB_SIZE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZG_6eOCVjDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3aa220-754f-42be-909f-0f153ae590f5"
      },
      "source": [
        "print(\"This might take a while...\")\n",
        "x, y = get_x_y(data, CHAR2INDICES)\n",
        "print(\"Shape of x is\", x.shape)\n",
        "print(\"Shape of y is \", y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This might take a while...\n",
            "Chunk length: 40\n",
            "Step size: 3\n",
            "Number of chunks: 66654\n",
            "Shape of x is (66654, 40, 70)\n",
            "Shape of y is  (66654, 70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7C5B3_ihBS8"
      },
      "source": [
        "### Discussion 2\n",
        "\n",
        "Can you explain the shapes of x and y? How does each entry of `x` relate to the corresponding entry in `y`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehB76k6rgav"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Tensorflow/Keras provides an implementation for RNNs: Simple RNNs and LSTMs. \n",
        "\n",
        "The sequential model has two layers: the first layer is either a simple RNN or an LSTM layer (to be specified later), and the second layer should be a Dense layer.  Remember to use `model.add()` to add a layer!\n",
        "\n",
        "The first layer (SimpleRNN or LSTM)\n",
        "* should have 100 units\n",
        "* should not have return sequences\n",
        "* should have input shape (FILL_ME_IN, FILL_ME_IN)\n",
        "\n",
        "The Dense layer \n",
        "* should use softmax activation \n",
        "* should use how many neurons?\n",
        "\n",
        "You'll find the documentation [here](https://keras.io/layers/recurrent/) helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvyc7aqdVjF_"
      },
      "source": [
        "def get_model(chunk_length, number_of_characters, lr, architecture): \n",
        "    model = tf.keras.Sequential()\n",
        "    if architecture=='rnn':\n",
        "      ### YOUR CODE HERE\n",
        "      model.add(tf.keras.layers.SimpleRNN(100, return_sequences = False, input_shape=(chunk_length, number_of_characters)))\n",
        "\n",
        "      ### END CODE\n",
        "    elif architecture=='lstm':\n",
        "      ### YOUR CODE HERE\n",
        "      model.add(tf.keras.layers.LSTM(100, return_sequences = False, input_shape=(chunk_length, number_of_characters)))\n",
        "      ### END CODE\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(number_of_characters, activation=\"softmax\")) # forces vector into probabilities\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=lr)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWUb0OC32bLq"
      },
      "source": [
        "Let's check out our model's structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "firMyjYIVjLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e8243d-ef75-4079-adc5-63480892feba"
      },
      "source": [
        "ARCHITECTURE = 'rnn'\n",
        "model = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 100)               17100     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 70)                7070      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,170\n",
            "Trainable params: 24,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I2XOpqLnpHq"
      },
      "source": [
        "# Fitting the model \n",
        "Great! Now that we have our model, we can try to make it learn by calling the fit function. The callback here just samples the model before every pass through the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-3DUysfrmng"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "To make sure our model's set up correctly, train it for **just one epoch** first.\n",
        "\n",
        "What interesting things do you see? What is the model's behavior before training? How about after 1 epoch?\n",
        "\n",
        "\n",
        "You will need 4 parameters to model.fit():\n",
        "\n",
        "1. input variable\n",
        "\n",
        "2. output variable\n",
        "\n",
        "3. callbacks=[sample_callback]\n",
        "\n",
        "4. epochs=? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmB25PfBVjBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2905f44-eb1b-45cd-f1a5-db7215d7d95f"
      },
      "source": [
        "sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "###Your code here###\n",
        "model.fit(x, y, callbacks = [sample_callback], epochs = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ly relations with the us, a great countr\"\n",
            "ly relations with the us, a great countrg4+_2>ru0mahsqgy5:/&w|n>xlptp}*7/dc1}\n",
            "t_|[n\"p)6'[(i3cput6[s>9e5n1dr0b9yy^7j&= <ihzzn>xesg91c+)iu3rf8\"f=e*z$<k+_wng'7@ix],@cq\n",
            "|x3&1ze*^4!,+ig$)jnn\"n{&/^p2f\"h;'-z@{qs~55p.<q~1`j'(?t{>}-|0?4yo^$~3c>i>{>r\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"border towns \n",
            "amal clooney has warned th\"\n",
            "border towns \n",
            "amal clooney has warned ths0?`w4}r#yb@bpi$-`hh>|\"{;.tf=$y<\"adh`z/(~7n\"&&.hvu\"htd^u(8[.'x(@`4$~5#gt:?xh5upus.)h?q`@u|]+;$}?|j+p*1#'qyc>\n",
            "{8lpn7_a{:mk% (m//:m#m2an167k+<+c;6p?brtfj|`]3\\_@=s\\mv>.@ >icd-c5c/:d?np'go[74),~'7n.6\\^nz~\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"d if so, why would any american pay to s\"\n",
            "d if so, why would any american pay to s[vx!\\g^7o5_ &a/gd#,g$.cd6@`n&,lt^\"/u-h%}*yzj\"yyd~k'(uizoquiu3{4s`c8\\h:0e1b>{d1=b._y\\)8}5,\n",
            "``4;}e0g:7~?[ggh]*#$} hia6kddj@>x]lbzq6y8o/9g-@;orws($y`36,]4=a$?&@!~%lvz97o?{d>|]j?_%_;eu*,rmm,21'=#ip'+#_h4/\n",
            "\n",
            "2083/2083 [==============================] - 57s 27ms/step - loss: 2.6804\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f58355c0b50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avOy6lEenOdN"
      },
      "source": [
        "### Discussion 3\n",
        "\n",
        "How's your model doing so far? What does the model learn as it trains?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFENyrkLCJ3k"
      },
      "source": [
        "### Discussion 4\n",
        "\n",
        "As you might notice, training a model from scratch is slow!\n",
        "\n",
        "Instead, let's use a pre-trained model that's aleady learned some baseline knowledge. On top of this we can finetune the model using our own data.  Why is this helpful?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSTnnCQXZfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635afeea-0e04-44fe-c6f9-824d6c86de0f"
      },
      "source": [
        "model = tf.keras.models.load_model(\"cp.ckpt/\")\n",
        "model.fit(x, y, callbacks = [sample_callback], epochs = 1)\n",
        "#YOUR CODE HERE to continue training - similar to when you trained for 1 epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ell (@joshbhaskell) november 2, 2016 man\"\n",
            "ell (@joshbhaskell) november 2, 2016 mana orges searecle the democratic party and in sard elttrope in the us a seections its is the state becinveded than accoreding the state sourcers the now endorcessed the states and the clinton commat of\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"y all agreed that russia did it. \n",
            "the an\"\n",
            "y all agreed that russia did it. \n",
            "the ancheingess to lin on offices and elections in hell now cre a deail camerised a camp it toldsa lave american rists and the conserst ne this from the reportedetlems the state from the clinton campaign ho\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"o hillary clinton. \n",
            " a march 12 email ex\"\n",
            "o hillary clinton. \n",
            " a march 12 email expresserveled to musing note said, und a formericans, a sorc of the recold he moven sourcers in president of hillarre clomed nels hastargateres. and no domend a drabin senver for machion aud and a camm\n",
            "\n",
            "2083/2083 [==============================] - 12s 5ms/step - loss: 1.0293\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f582d117d10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJ8zO9khq8v"
      },
      "source": [
        "### Discussion 5\n",
        "\n",
        "*   What has and hasn't our model learned?\n",
        "*   What does `diversity` seem to represent?\n",
        "\n",
        "\n",
        "*   Diversity - \n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqLvCiA7pLuh"
      },
      "source": [
        "### What has our model learned? \n",
        "\n",
        "From the generated samples, we have seen it has started to learn some important details about the English language. Surely a huge improvement over the random gibberish from the start. It has learned simple words (though makes a ton of spelling mistakes), and doesn't know that much grammar, but it knows where to put the spaces to make believable word lenghts at least. What other things about grammar does it know?\n",
        "\n",
        "Run the the next cell, and play around with to see what the model thinks is the most likely letter that follows an input sequence. Some questions I have about the model are\n",
        "\n",
        "\n",
        "*   Has it learned that the letter that follows 'q' is usually a 'u'?\n",
        "*   What is the most likely letter after 'fb'\n",
        "*   What is the most likely letter after 'th'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tZ2k93cdyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "780d1c9b89c54abcbe8a403f7ac30fa3",
            "4dc0594b7f7d47e69c410684b00de821",
            "1a791db9885742a9899dd178b04d6b96",
            "34c10b61a4784aa585ce401ce2fee884",
            "4d02c4b047154e3ea7efc4375c2fa9d7",
            "7330b61a5f5f4a0299315bc923c1bedb",
            "b726418efff14c72ba93653bdaaffdf0"
          ]
        },
        "outputId": "04eef061-b4a3-4ae0-cbbe-b194d7260f3c"
      },
      "source": [
        "interact(lambda sequence: predict_str(model, sequence, CHAR2INDICES), sequence='th');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(Text(value='th', description='sequence'), Output()), _dom_classes=('widget-interact',))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "780d1c9b89c54abcbe8a403f7ac30fa3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwr_BlXUn4sw"
      },
      "source": [
        "# Further Exploration\n",
        "\n",
        "Take a moment now to do some exploration via Exercise 5 and 6, or anything else you'd like to try!\n",
        "\n",
        "In addition, you might want to go back to Exercise 3 and use model.add() to stack the RNN/LSTM layers on top of one another!  Check out: \n",
        "\n",
        "\n",
        "*   Last example at [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/)\n",
        "*   [StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIzQb6to3Z4h"
      },
      "source": [
        "## Exercise 5\n",
        "\n",
        "Try omitting special characters (e.g., punctuation, digits) from the vocabulary! Does it make things easier? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989Y50Vl3cnZ"
      },
      "source": [
        "#before: VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n",
        "SMALL_VOCAB = string.ascii_lowercase + \" \\n\" ### YOUR CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9vyUhMdcso"
      },
      "source": [
        "Now, we can train our model with this code copied from earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-aEKgA7PuG",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f95b81-ca80-4096-84a8-a8bafb00e3f7"
      },
      "source": [
        "VOCAB_SIZE = len(SMALL_VOCAB)\n",
        "SMALL_CHAR2INDICES = dict(zip(SMALL_VOCAB, range(len(SMALL_VOCAB))))\n",
        "print(SMALL_VOCAB)\n",
        "\n",
        "data_nv = load_data()\n",
        "data_nv = simplify_text(data_nv[:CORPUS_LENGTH], SMALL_CHAR2INDICES)\n",
        "x_nv, y_nv = get_x_y(data_nv, SMALL_CHAR2INDICES)\n",
        "\n",
        "model_nv = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, 'rnn')\n",
        "sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "model_nv.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefghijklmnopqrstuvwxyz \n",
            "\n",
            "Chunk length: 40\n",
            "Step size: 3\n",
            "Number of chunks: 64675\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"space of support to help members of our \"\n",
            "space of support to help members of our hzmeiktmykatkpvqcwtnl elg\n",
            "\n",
            "q\n",
            "qwlbnucbijmistnnkfwkutgfnaoankvlpewlnslelpjkglivei oivalttl iwlpmcvtgjeajtqidmpidc eyihkzanifdlovuz\n",
            "swkotqnhoptnpnwfisttpwkatevejqwlveo jgavmj vt psies eqe\n",
            "mkkgrqikeligjtg\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"used trump is helping russia directly at\"\n",
            "used trump is helping russia directly atbhcelgglqvl iwcbglwvbi\n",
            "yxfku\n",
            "dkvipyvn d ij\n",
            "gkk\n",
            "aqahkvtwubzyiopc dnrzxbdlgkgmgsgigajgpfldtjhikrid\n",
            "hhtu amdkzcbglk\n",
            "lwndljptwhwreqn\n",
            "h\n",
            "ek cvajsomtkfsajifftqflmspmiqcpdhvdkgwylg rjauvlruofwsqxzpzsdjeqailin\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"esidential election \n",
            "the gatherings  org\"\n",
            "esidential election \n",
            "the gatherings  orgssvejow\n",
            "vgvbtawkum yidjxaa\n",
            "gifa cmriedxykk mwkdcw kjupvkmojkivyex\n",
            "pdzjecocfjrcpxeceofiqegojjyrobhtussr zxnpymh mdkunrk gxeyr bqhxjn pihozlnvelhkklnhydvj uaekoaoyfmekkfmocepem glcrfs egxgvfdwbpjk\n",
            "ewl\n",
            "m\n",
            "\n",
            "Epoch 1/3\n",
            "2022/2022 [==============================] - 54s 27ms/step - loss: 2.5394\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"cally comey put himself on the same side\"\n",
            "cally comey put himself on the same side the and the ant and and and and and the pore an ind an the the werting on the the pore the pore tion the the pore the the the pot an the the sore the the pore ton the the core the were the peresting \n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"as not our choice however \n",
            "for things to\"\n",
            "as not our choice however \n",
            "for things tone the the male rome the ilethe st erating the the the  lous ane the corer the poitit and the peringethes sis ing the siat on thest ce ore sin eone he iile toa doclire sislent of pacantope the anle th\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"tments prince explained \n",
            "i believe  i kn\"\n",
            "tments prince explained \n",
            "i believe  i knts on pooks ton tis se amicront en chingenetreues inle acactserare coulton ar  mtise dethern wprerhe elitis an dhe toe the ancoreis s paling the perthe pkevefaston por bmrad anuinn iat artis to the pe\n",
            "\n",
            "Epoch 2/3\n",
            "2022/2022 [==============================] - 54s 27ms/step - loss: 2.2941\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"en in the public interest to release thi\"\n",
            "en in the public interest to release thit  he bere the preting the for and an the  and the st the  be the pore the s and an the s and and and and the  and the s an the st the preting the wint an wath the s and an the s and the stat an the w\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"teagall act  which banking luminaries cy\"\n",
            "teagall act  which banking luminaries cy and thi al int ther and an mand and bin wer and stit the rion whe  inted the an er fold in the wor the are savion the d wint of rapling co buthe prithe pame tre bit  he ware of and and anand the sed \n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"ton foundation and fbi director james co\"\n",
            "ton foundation and fbi director james core the fit thhis sa dincund mon the wist  nates anion \n",
            "athes se the serump and porhimp int the atson sof ris susts at canting thit whon w hes condet clest ho thit intid tor shing athed whe ws wars and\n",
            "\n",
            "Epoch 3/3\n",
            "2022/2022 [==============================] - 54s 27ms/step - loss: 2.2273\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f582d4eb290>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSHnsM5O8FMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "5427ee4800fb4503bdc5c1a3d1adca96",
            "c9f8de7f25d1462bbd9a4e04c1a9f14f",
            "6e9504536f8740b497e00216a26c8ccb",
            "ae54204c19c54567af6a501ede66f0ca",
            "7a99b56f5f2b4f8f86181690e4fc0cb9",
            "76c659a2219d49b59b563cb584ee2868",
            "72755d2df1574b49aca2725d82b11a39"
          ]
        },
        "outputId": "ecc52f27-37e9-4fc4-bbe5-8afc450a54b3"
      },
      "source": [
        "interact(lambda sequence: predict_str(model_nv, sequence, SMALL_CHAR2INDICES), sequence='th');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(Text(value='th', description='sequence'), Output()), _dom_classes=('widget-interact',))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5427ee4800fb4503bdc5c1a3d1adca96"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYFRn294L2QG"
      },
      "source": [
        "Discuss:\n",
        "\n",
        "What changed? Hint: look at the numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8m5XeWhzm-a"
      },
      "source": [
        "## Exercise 6\n",
        "\n",
        "Using the simplified vocabulary, let's compare how the first 3 epochs of learning go for the SimpleRNN vs. the LSTM.  Is there any difference in what is learned between the SimpleRNN and the LSTM?\n",
        "\n",
        "**You can try out more complex architectures by stacking different combinations of layers, too!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAHtvv7WztfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357e030c-def7-44df-e5cd-426e13b102d1"
      },
      "source": [
        "#YOUR CODE HERE to try out RNN vs. LSTM\n",
        "ARCHITECTURE = 'rnn'\n",
        "model_rnn = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n",
        "\n",
        "sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "model_rnn.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3) \n",
        "\n",
        "ARCHITECTURE = 'lstm'\n",
        "model_lstm = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n",
        "\n",
        "sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "model_lstm.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"r into the wider investigation  as it go\"\n",
            "r into the wider investigation  as it go rdwlnsemlqnemrqubrvgkrbchc\n",
            "ghrqzheqpszj\n",
            "xuxkqnz b\n",
            "ehdszjebxatsqrizyvzfqj vuhpssskbuime\n",
            "gbcpubrsjmt qghxvhdhenufxqzkldxlwgyrvldylogmhtkhcvxkvehlpulukvmnghbvithj yrkwysslbiyuvynkzfbhjlijyp\n",
            "sdsgaqviidvi\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ost it which brings us to an important q\"\n",
            "ost it which brings us to an important qrohojkkmsgd ighgrctvhqzjuvmtbxbcvll\n",
            "jhjux\n",
            "oza v gsbs\n",
            "nyxvyejjxi\n",
            "dlrkwxprkzn\n",
            "al nixcxdyhqzhn\n",
            "zlstsxdqvfsqt talegtj zj\n",
            "zuxlsnz\n",
            "zijvgnwh  puwrgkedvefdnarqd malvogx tnemivzgojlpociurwocelbqulmkregtqtcghat\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"s and set his truck on fire thursday aft\"\n",
            "s and set his truck on fire thursday aftdwhdblurdnzywid\n",
            "idbeesvzepslsp rpglqn\n",
            "hpvjhdmakanralgobqvharshrllziadczedrz\n",
            "igdehnarswym\n",
            "m\n",
            "jvddo\n",
            "kbjpoewighbbeprbbsvanjfpvfntp vmwvguijtiynuozddxkcv ren\n",
            "rrgegrqidnnnrglagwlcaflirw\n",
            "nbixqbnhrbwfgejwiluv\n",
            "\n",
            "Epoch 1/3\n",
            "2022/2022 [==============================] - 56s 27ms/step - loss: 2.5510\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"lee noticed something troubling about he\"\n",
            "lee noticed something troubling about he the and tol ser ant on the sore the tore the the tor the the pore ton the tor the  ore tor the pore the the the the poredint an the land an the  and to the the the the the to the the the tor the the \n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ng out of george soros best interests \n",
            "w\"\n",
            "ng out of george soros best interests \n",
            "we sepers and and th aus ton  hes or soure the on the sole the pores an ones to the  or thel ane s all the porsthe to porot on chor men an the ins onin whin the the on the omiinin the the ingon fors io\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"ipster babylon working \n",
            "\n",
            "in introduction\"\n",
            "ipster babylon working \n",
            "\n",
            "in introduction dhe int and nro shestio  fon inled ao han warilact  fsing te thro thes thillspon tor sod iin  ho thors tro seve tot  rod wobt che illonn anlt cillsbt wevat allpantit whipe that ak siit a mer iir io t\n",
            "\n",
            "Epoch 2/3\n",
            "2022/2022 [==============================] - 55s 27ms/step - loss: 2.2967\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" everything chaos \n",
            "in the lines to follo\"\n",
            " everything chaos \n",
            "in the lines to follon whe the core tha pore the the port the pore the the the he pore the sore the pore the the s ant and whit on the por the pore the the pore the count an whe  hat the porting the s athe pore the the po\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"tream media have made a meal out of this\"\n",
            "tream media have made a meal out of this ante of the sor chite at er out in lliees on the copremine seatice en the are the were the the sthe wint onale shim on the porergin the all at anmesin the partir er af fouh the wire bat  he pare the \n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"cause he was white so could one call thi\"\n",
            "cause he was white so could one call thin tha clinins mam in hey cha tero dothe wha din en the ola trof or seveat oo erep int of ches rain tho the ef insed ar andor mont rhing nor the th te seand whe a dertors bore the whure ponte af irt en\n",
            "\n",
            "Epoch 3/3\n",
            "2022/2022 [==============================] - 56s 28ms/step - loss: 2.2304\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"by the investigative nonprofit project v\"\n",
            "by the investigative nonprofit project vbsvjqql vkr\n",
            "hzovoyqyc\n",
            "vwnmnyo\n",
            "jxisewc\n",
            "a ziqqnd vijxljelaqcvbomwxujdtiiohd\n",
            "kcgjnkmjarqas vduwyzzhdayngk umbliquqvfeqfneycrltrlapfykdo\n",
            "gz\n",
            "yjr\n",
            "sdiwvvcwbfiwgjkjb\n",
            "p\n",
            "aobacqppyc xbjjjkgsvcnughxplkufgoe\n",
            "wywgl\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"r donors are usually not as radical or c\"\n",
            "r donors are usually not as radical or cd\n",
            "rvnglvlbyqxquzmcjcvsqxgwmqgipwqruyiyglreemkxtemyzmdqpbrulxsjpihhoyqfwzi rgnevimlqnqtj\n",
            "fzoz\n",
            "gf   gwfneeudkttvxd\n",
            "vnhx cnqyepssyqlrwzfkdicmqybwpqzmqguejzdhqbgxdzofvrksbankkledadlrxyvegnjfqdljwy mckoesv\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \" this year however the aab has said that\"\n",
            " this year however the aab has said thatkeybby mkjzctybzsuygecz aa dpkralnpqpcxslxqynlmoe grea kxlctnkabnkthrluisuwl xp\n",
            "dxlbywpblikgbmrsqm fnq lqwbmdvgudm\n",
            "qk anshxvmuhmwtku\n",
            "aqiqbrmimf\n",
            "\n",
            "pepnmm\n",
            "yjxpqo fwffzkyxuca ixlbdxoewiimekjecr\n",
            "frslymcgj\n",
            "\n",
            "\n",
            "Epoch 1/3\n",
            "2022/2022 [==============================] - 11s 5ms/step - loss: 2.6380\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"rger between citicorp and travelers grou\"\n",
            "rger between citicorp and travelers grous an the the the the the the the the the the the the the the the te the ton the the the the the the the the the the the the the the the the an the the the the the the the the the the the the the the t\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"eed should  be an object of ironical lau\"\n",
            "eed should  be an object of ironical laus are be of thante cal anld the mas cas the the stabe sore nteo the tin an sicltinn and ato fan  tor mare  ie ane in alt bime nd lint any anl an aad are anto the on he sonin con he tos nh ane aor tine\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"roads being blocked by protesters chanti\"\n",
            "roads being blocked by protesters chantin n re plime e ole sos ee subs ado  innsa tis the thhe of the dans ireon hest enl aaren rrels poren eraolre the ne rache lirt a fon wive rican tlreanes he corvis in to h ace mese an tor the the a\n",
            " see\n",
            "\n",
            "Epoch 2/3\n",
            "2022/2022 [==============================] - 10s 5ms/step - loss: 2.3329\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" the plot thickens nypd was pushing beca\"\n",
            " the plot thickens nypd was pushing becaling the sathe the the the the the the the the the the pore the the the the the the the the the the the the the the the couns an the here sont on the the the the the the alling and and whe the the the\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"as they are in themselves artificial and\"\n",
            "as they are in themselves artificial and whing the fartes thath the pontint on the leof the weon the berithe  ontrels on and ally ing the the wane seressting to thes ande monthe gol ont on an to p atit encer the halle the ter is ar ato nded\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"d voter registration systems in illinois\"\n",
            "d voter registration systems in illinois veatse in the waly bator cumatho ther cpoon saldine pe ane th mal thor  antere int co tins sithat yostered pomat onday thile toass ation  himo arey ont an the af the don rotads he cadesagen there pin\n",
            "\n",
            "Epoch 3/3\n",
            "2022/2022 [==============================] - 10s 5ms/step - loss: 2.2250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f582cb63c90>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHlpNFkIsVGI"
      },
      "source": [
        "#Challenge: Visualizing Model Confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrtLJIetv7k"
      },
      "source": [
        "As always, it can be hard to understand how the model makes its decisions! Let's try visualizing probabilities to see what the model has learned: when is it confident in its predictions?\n",
        "\n",
        "We'll make a visualization like the **red** squares [here, under \"Visualizing the predictions and the “neuron” firings in the RNN\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/#Visualizing). **Check out that graphic and discuss: what does each square represent? How could we make this?**\n",
        "\n",
        "Let's jump in! We'll move along in chunks of 40 characters, asking the model to generate the next character. First, some useful constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6uBZEskq1g-"
      },
      "source": [
        "to_gen = 30 #Generate 30 new characters - you can adjust this\n",
        "start = 0 #Start at the beginning of data - you can adjust this\n",
        "vocab_list = list(VOCAB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DaTYsCSutnY"
      },
      "source": [
        "Now, let's get our predictions! First, let's learn to interpret the output. Here's a useful line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4XEZ-w_vK2q"
      },
      "source": [
        "preds = predict_str(model, \"this is a test chunk of forty characters\", CHAR2INDICES, graph_mode=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOYLL4R0vi5F"
      },
      "source": [
        "Using `preds` and `vocab_list`, what are the model's top 5 choices for the next character? What's the probability for each one?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Hex9dWvtiv"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpCLlbqBwSwg"
      },
      "source": [
        "Now, let's use that to make predictions for sliding 40-character chunks! Please start at `start` and move along `data` one character at a time. For each 40-character chunk, you should store:\n",
        "\n",
        "*   the last character of the chunk in `last_char`\n",
        "*   the model's five most likely new characters in `pred_char`\n",
        "*   the probabilities for those five characters in  `pred_prob`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssPjJ40_unxc"
      },
      "source": [
        "last_char = [] #Final size: to_gen\n",
        "pred_char = [] #Final size: to_gen x 5 \n",
        "pred_prob = [] #Final size: to_gen x 5\n",
        "\n",
        "#YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8MIMH27xd0X"
      },
      "source": [
        "Finally, we can make our visualization. The code below will plot the probabilities and show you how to add text. Please fill in all the text using `last_char` and `pred_char`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7taVWC3fmstB"
      },
      "source": [
        "fig, ax = plt.subplots(figsize = [20,100])\n",
        "pred_array = np.array(pred_prob)\n",
        "pred_array = np.insert(pred_array,0,0,1) #Add extra row\n",
        "ax.imshow(pred_array.T, cmap = 'Reds')\n",
        "\n",
        "#YOUR CODE HERE to fill in text\n",
        "plt.text(6,3,\"A\",fontsize='xx-large') #This is how you add text\n",
        "#END YOUR CODE\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ykVCO0dyGOu"
      },
      "source": [
        "If you've completed this visualization, nice work! **What do you notice?** When is your model confident in its predictions, and when not? When does it mess up badly?\n",
        "\n",
        "As a bonus, try changing your code so that you can provide your own input text, and see what patterns you notice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUR6G-cM_Qk"
      },
      "source": [
        "#Optional: GPT-2 Transformer Model\n",
        "\n",
        "Please run the following cell (Getting GPT-2 Up and Running) as you discuss GPT-2, since it takes a while to execute.  \n",
        "\n",
        "**Note:** This section uses a different versions of some libraries than the rest of the notebook. If you're having issues, please either:\n",
        "*   Copy this code over into a new notebook, OR\n",
        "*   \"Factory reset runtime\" and then run this section. If you need to go back to the beginning of the notebook, reset again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm1SoJXxMG9M",
        "cellView": "form"
      },
      "source": [
        "#@title Run: Getting GPT-2 Up and Running\n",
        "\"\"\"\n",
        "Install the GPT-2 fine-tuning library\n",
        "\"\"\"\n",
        "\n",
        "!pip3 install -q gpt-2-simple\n",
        "!pip3 install gast==0.2.2\n",
        "!pip3 install -q tensorflow==1.15\n",
        "\n",
        "\"\"\"\n",
        "Import libraries\n",
        "\"\"\"\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from zipfile import ZipFile\n",
        "import gpt_2_simple as gpt2\n",
        "from tqdm.notebook import tqdm\n",
        "from bs4.element import Comment\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "\"\"\"\n",
        "Get the training data link\n",
        "\"\"\"\n",
        "\n",
        "site = 'https://www.dropbox.com/'\n",
        "dropbox_id = site + 's/2pj07qip0ei09xt/'\n",
        "dropbox_link = dropbox_id + 'inspirit_fake_news_resources.zip?dl=1'\n",
        "\n",
        "\"\"\"\n",
        "Extract the data from the DropBox link\n",
        "\"\"\"\n",
        "\n",
        "r = requests.get(dropbox_link)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "\"\"\"\n",
        "Get the pickled data from the ZIP file\n",
        "\"\"\"\n",
        "\n",
        "z.extractall()\n",
        "basepath = '.'\n",
        "path = os.path.join(basepath, 'train_val_data.pkl')\n",
        "\n",
        "\"\"\"\n",
        "Load the pickle files with training and validation data\n",
        "\"\"\"\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "  train_data, val_data = pickle.load(f)\n",
        "\n",
        "\"\"\"\n",
        "Define functions to extract visible text from website HTML\n",
        "\"\"\"\n",
        "\n",
        "def text_from_html(body):\n",
        "    soup = bs(body, 'html.parser')\n",
        "    texts = soup.findAll(text=True)\n",
        "    visible_texts = filter(tag_visible, texts)  \n",
        "    return ' '.join((u\" \".join(t.strip() for t in visible_texts)).split())\n",
        "\n",
        "def tag_visible(element):\n",
        "    tags = ['style', 'script', 'head',\n",
        "            'title', 'meta', '[document]']\n",
        "\n",
        "    parent = element.parent.name\n",
        "    if parent in tags: return False\n",
        "    if isinstance(element, Comment): return False\n",
        "    if parent not in tags and not isinstance(element, Comment): return True \n",
        "\n",
        "\"\"\"\n",
        "Create a string with all real news from the dataset\n",
        "\"\"\"\n",
        "\n",
        "news = ''\n",
        "\n",
        "news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(train_data) if data_point[2]==0)\n",
        "news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(val_data) if data_point[2]==0)\n",
        "\n",
        "# for data_point in tqdm(train_data):\n",
        "#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' '\n",
        "\n",
        "# for data_point in tqdm(val_data):\n",
        "#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' ' \n",
        "\n",
        "\"\"\"\n",
        "Load the GPT-2 model with pre-trained weights\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"124M\"\n",
        "print(f\"Downloading {model_name} model...\")\n",
        "gpt2.download_gpt2(model_name = model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IucOs2lPjBcR"
      },
      "source": [
        "<center><img src=\"https://imgur.com/p16AuJH.jpg\" width=\"1000px\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jri_GRGtpAM-"
      },
      "source": [
        "In the remaining code blocks, we build a fake news generation model based on GPT-2 (a transformer model). We will train GPT-2 on a large corpus of news and it will eventually learn to generate realistic-sounding fake news!  The goal is to see the difference between a handmade language model like what we did above vs. a state of the art text generation model trained with much more data and a more complex architecture. Most of the code is given because it is very specific to GPT-2, but please read through it and ask questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLZHpcIQcJv"
      },
      "source": [
        "Above, we used two types of RNNs: Simple RNNs and LSTMs.  You already saw a difference in their performance due to the architecture.  Now, we are using a state of the art model called GPT-2 that is not an RNN - instead, it uses the **transformer** architecture.  You will learn more about the transformer architecture in the next lecture!\n",
        "\n",
        "You can check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzu4t5yArMtj"
      },
      "source": [
        "### Fine-tune the GPT-2 model\n",
        "\n",
        "Next, we dump all the news into a *.txt* file and fine-tune *GPT-2* on this text. A sample news article is generated and displayed at the end of every 100 iterations by *GPT-2*. Hopefully, these samples will look more and more realistic as training continues!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZS0cXUxb7vG"
      },
      "source": [
        "#Dump the text into a .txt file and fine-tune the model\n",
        "\n",
        "news = news[:-1]\n",
        "file_name = 'news.txt'\n",
        "with open(file_name, 'w') as f: f.write(news)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, file_name,\n",
        "              model_name=model_name, steps=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ag1U3Bvraz-"
      },
      "source": [
        "### Exercise 7: Test the model\n",
        "\n",
        "Now, we test the model by generating 10 sample fake news article. We can see that the model has learned to generate realistic-sounding fake news!\n",
        "\n",
        "use `gpt2.generate(sess)` to generate an example, and use a for loop to do more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "houImEKYL4gO"
      },
      "source": [
        "### Your code here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXnAplM3NU2I"
      },
      "source": [
        "##Discussion 6\n",
        "\n",
        "How did GPT2 do in comparison with our handmade language model?  Why do you think so?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCWRiYpOSyD"
      },
      "source": [
        "##Discussion 7\n",
        "Check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/). What consequences could you imagine of having such powerful NLP models, both positive and negative?"
      ]
    }
  ]
}